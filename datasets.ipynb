{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e404604d",
   "metadata": {},
   "source": [
    "# BEA-2019 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4dcb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 68616 Dev pairs: 8768\n"
     ]
    }
   ],
   "source": [
    "import re, os, pandas as pd\n",
    "\n",
    "# parse an \"A ...\" line from .m2\n",
    "A_RE = re.compile(r\"^A (\\d+) (\\d+)\\|\\|\\|[^|]*\\|\\|\\|([^|]*)\\|\\|\\|\")\n",
    "\n",
    "def apply_edits(src):\n",
    "    toks = src.split()\n",
    "    # apply collected edits (right→left so indices stay valid)\n",
    "    for s,e,repl in sorted(apply_edits.edits, key=lambda x: x[0], reverse=True):\n",
    "        repl_toks = [] if repl in (\"\", \"-NONE-\") else repl.split()\n",
    "        toks[s:e] = repl_toks\n",
    "    return \" \".join(toks)\n",
    "apply_edits.edits = []  # static holder\n",
    "\n",
    "def m2_to_pairs(path):\n",
    "    pairs, src = [], None\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\"S \"):\n",
    "                # flush previous\n",
    "                if src is not None:\n",
    "                    tgt = apply_edits(src)\n",
    "                    pairs.append((src, tgt))\n",
    "                src = line[2:]\n",
    "                apply_edits.edits = []\n",
    "            elif line.startswith(\"A \"):\n",
    "                m = A_RE.match(line)\n",
    "                if m:\n",
    "                    s, e, repl = int(m.group(1)), int(m.group(2)), m.group(3).strip()\n",
    "                    apply_edits.edits.append((s, e, repl))\n",
    "            elif line == \"\":  # sentence boundary\n",
    "                if src is not None:\n",
    "                    tgt = apply_edits(src)\n",
    "                    pairs.append((src, tgt))\n",
    "                    src = None\n",
    "                    apply_edits.edits = []\n",
    "    # tail\n",
    "    if src is not None:\n",
    "        tgt = apply_edits(src)\n",
    "        pairs.append((src, tgt))\n",
    "    return pairs\n",
    "\n",
    "# ---- collect train/dev across files ----\n",
    "m2_dir = \"data/wi_locness/m2\"\n",
    "train, dev = [], []\n",
    "for fname in os.listdir(m2_dir):\n",
    "    if fname.endswith(\".m2\"):\n",
    "        path = os.path.join(m2_dir, fname)\n",
    "        if \"train\" in fname:\n",
    "            train += m2_to_pairs(path)\n",
    "        elif \"dev\" in fname:\n",
    "            dev += m2_to_pairs(path)\n",
    "\n",
    "pd.DataFrame(train, columns=[\"input_text\",\"target_text\"]).to_csv(\"bea_train.csv\", index=False)\n",
    "pd.DataFrame(dev,   columns=[\"input_text\",\"target_text\"]).to_csv(\"bea_dev.csv\",   index=False)\n",
    "\n",
    "print(\"Train pairs:\", len(train), \"Dev pairs:\", len(dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(train_pairs, columns=[\"input_text\", \"target_text\"])\n",
    "df.to_csv(\"bea_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f92ce",
   "metadata": {},
   "source": [
    "# BEA-2019 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8776f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e833d",
   "metadata": {},
   "source": [
    "# JFLEG (Similar to CONNL but CONNL is Part of NUCLE and We Don't Have Access Yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5322a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'corrections'],\n",
       "    num_rows: 748\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jfleg = load_dataset(\"jfleg\", split=\"test\")  \n",
    "jfleg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6b4c6",
   "metadata": {},
   "source": [
    "# WikiAuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5cb41ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['alignment_label', 'normal_sentence_id', 'simple_sentence_id', 'normal_sentence', 'simple_sentence', 'gleu_score'],\n",
       "        num_rows: 373801\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['alignment_label', 'normal_sentence_id', 'simple_sentence_id', 'normal_sentence', 'simple_sentence', 'gleu_score'],\n",
       "        num_rows: 73249\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['alignment_label', 'normal_sentence_id', 'simple_sentence_id', 'normal_sentence', 'simple_sentence', 'gleu_score'],\n",
       "        num_rows: 118074\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_auto = load_dataset(\n",
    "    \"chaojiang06/wiki_auto\",\n",
    "    \"default\",\n",
    "    revision=\"refs/convert/parquet\"   # <-- avoids script\n",
    ")\n",
    "\n",
    "wiki_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e027a1c",
   "metadata": {},
   "source": [
    "# ASSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19461646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['original', 'simplifications'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'simplifications'],\n",
       "        num_rows: 359\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset = load_dataset(\"asset\")\n",
    "asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcc4de",
   "metadata": {},
   "source": [
    "# Save Datasets for HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9efcf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets to hpc_datasets/\n",
      "✓ BEA-2019 train set already saved as bea_train.csv\n",
      "✓ BEA-2019 validation set already saved as bea_dev.csv\n",
      "\n",
      "Loading JFLEG dataset...\n",
      "✓ Saved JFLEG test set (748 examples)\n",
      "\n",
      "Loading WikiAuto dataset...\n",
      "✓ Saved WikiAuto train set (373801 examples)\n",
      "\n",
      "Loading ASSET dataset...\n",
      "✓ Saved ASSET validation set (2000 examples)\n",
      "✓ Saved ASSET test set (359 examples)\n",
      "\n",
      "==================================================\n",
      "All datasets saved successfully!\n",
      "==================================================\n",
      "\n",
      "Files ready for HPC transfer:\n",
      "  - bea_train.csv\n",
      "  - bea_dev.csv\n",
      "  - hpc_datasets/jfleg_test.csv\n",
      "  - hpc_datasets/wikiauto_train.csv\n",
      "  - hpc_datasets/asset_validation.csv\n",
      "  - hpc_datasets/asset_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create output directory for HPC datasets\n",
    "output_dir = \"hpc_datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving datasets to {output_dir}/\")\n",
    "\n",
    "# 1. BEA-2019 train set (already created as bea_train.csv)\n",
    "print(\"✓ BEA-2019 train set already saved as bea_train.csv\")\n",
    "\n",
    "# 2. BEA-2019 validation set (already created as bea_dev.csv)\n",
    "print(\"✓ BEA-2019 validation set already saved as bea_dev.csv\")\n",
    "\n",
    "# 3. JFLEG set\n",
    "print(\"\\nLoading JFLEG dataset...\")\n",
    "jfleg = load_dataset(\"jfleg\", split=\"test\")\n",
    "jfleg_df = pd.DataFrame({\n",
    "    \"input_text\": jfleg[\"sentence\"],\n",
    "    \"target_text\": [refs[0] for refs in jfleg[\"corrections\"]]  # Using first correction\n",
    "})\n",
    "jfleg_df.to_csv(os.path.join(output_dir, \"jfleg_test.csv\"), index=False)\n",
    "print(f\"✓ Saved JFLEG test set ({len(jfleg_df)} examples)\")\n",
    "\n",
    "# 4. WikiAuto train set\n",
    "print(\"\\nLoading WikiAuto dataset...\")\n",
    "wiki_auto = load_dataset(\n",
    "    \"chaojiang06/wiki_auto\",\n",
    "    \"default\",\n",
    "    revision=\"refs/convert/parquet\"\n",
    ")\n",
    "wiki_train_df = pd.DataFrame({\n",
    "    \"input_text\": wiki_auto[\"train\"][\"normal_sentence\"],\n",
    "    \"target_text\": wiki_auto[\"train\"][\"simple_sentence\"]\n",
    "})\n",
    "wiki_train_df.to_csv(os.path.join(output_dir, \"wikiauto_train.csv\"), index=False)\n",
    "print(f\"✓ Saved WikiAuto train set ({len(wiki_train_df)} examples)\")\n",
    "\n",
    "# 5. ASSET validation set\n",
    "print(\"\\nLoading ASSET dataset...\")\n",
    "asset = load_dataset(\"asset\")\n",
    "asset_val_df = pd.DataFrame({\n",
    "    \"input_text\": asset[\"validation\"][\"original\"],\n",
    "    \"target_text\": [refs[0] for refs in asset[\"validation\"][\"simplifications\"]]  # Using first simplification\n",
    "})\n",
    "asset_val_df.to_csv(os.path.join(output_dir, \"asset_validation.csv\"), index=False)\n",
    "print(f\"✓ Saved ASSET validation set ({len(asset_val_df)} examples)\")\n",
    "\n",
    "# 6. ASSET test set\n",
    "asset_test_df = pd.DataFrame({\n",
    "    \"input_text\": asset[\"test\"][\"original\"],\n",
    "    \"target_text\": [refs[0] for refs in asset[\"test\"][\"simplifications\"]]  # Using first simplification\n",
    "})\n",
    "asset_test_df.to_csv(os.path.join(output_dir, \"asset_test.csv\"), index=False)\n",
    "print(f\"✓ Saved ASSET test set ({len(asset_test_df)} examples)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All datasets saved successfully!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nFiles ready for HPC transfer:\")\n",
    "print(f\"  - bea_train.csv\")\n",
    "print(f\"  - bea_dev.csv\")\n",
    "print(f\"  - {output_dir}/jfleg_test.csv\")\n",
    "print(f\"  - {output_dir}/wikiauto_train.csv\")\n",
    "print(f\"  - {output_dir}/asset_validation.csv\")\n",
    "print(f\"  - {output_dir}/asset_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6fb1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiAuto train columns: ['alignment_label', 'normal_sentence_id', 'simple_sentence_id', 'normal_sentence', 'simple_sentence', 'gleu_score']\n",
      "\n",
      "First example:\n",
      "{'alignment_label': 1, 'normal_sentence_id': '0_66252-1-0-0', 'simple_sentence_id': '0_66252-0-0-0', 'normal_sentence': 'The Local Government Act 1985 is an Act of Parliament in the United Kingdom.', 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.', 'gleu_score': 0.800000011920929}\n"
     ]
    }
   ],
   "source": [
    "# Check WikiAuto column names\n",
    "from datasets import load_dataset\n",
    "wiki_auto = load_dataset(\"chaojiang06/wiki_auto\", \"default\", revision=\"refs/convert/parquet\")\n",
    "print(\"WikiAuto train columns:\", wiki_auto[\"train\"].column_names)\n",
    "print(\"\\nFirst example:\")\n",
    "print(wiki_auto[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f564ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JFLEG columns: ['sentence', 'corrections']\n",
      "\n",
      "First JFLEG example:\n",
      "{'sentence': 'New and new technology has been introduced to the society .', 'corrections': ['New technology has been introduced to society .', 'New technology has been introduced into the society .', 'Newer and newer technology has been introduced into society .', 'Newer and newer technology has been introduced to the society .']}\n",
      "\n",
      "\n",
      "ASSET validation columns: ['original', 'simplifications']\n",
      "\n",
      "First ASSET validation example:\n",
      "{'original': 'Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).', 'simplifications': ['countries next to it are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.', 'Nearby counties are Marin, Mendocino, Lake, Napa, and Solano and Contra Costa.', 'Adjacent counties are Marin, Mendocino, Lake, Napa, Solano and Contra Costa.', 'Neighboring counties are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.', 'Adjacent counties are Marin (south), Mendocino (north), Lake (northeast), and Napa (east). Solano and Contra Costa are to the southeast.', 'Counties next to it are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).', 'Marin, Mendocino, Lake, Napa, Solano, and Contra Costa counties are next to it.', 'Adjacent counties are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.', 'Counties next door are Marin (south), Mendocino (north), Lake (northeast), Napa (east), and Solano and Contra Costa (southeast).', 'Nearby counties are Marin (to the south), Mendocino (to the north), Lake (to the north-east), Napa (to the east), and Solano and Contra Costa (to the south-east).']}\n"
     ]
    }
   ],
   "source": [
    "# Check JFLEG and ASSET column names\n",
    "jfleg = load_dataset(\"jfleg\", split=\"test\")\n",
    "print(\"JFLEG columns:\", jfleg.column_names)\n",
    "print(\"\\nFirst JFLEG example:\")\n",
    "print(jfleg[0])\n",
    "\n",
    "asset = load_dataset(\"asset\")\n",
    "print(\"\\n\\nASSET validation columns:\", asset[\"validation\"].column_names)\n",
    "print(\"\\nFirst ASSET validation example:\")\n",
    "print(asset[\"validation\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9ba740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Files Summary:\n",
      "======================================================================\n",
      "✓ bea_train.csv                                 12.34 MB\n",
      "✓ bea_dev.csv                                    1.73 MB\n",
      "✓ hpc_datasets/jfleg_test.csv                    0.14 MB\n",
      "✓ hpc_datasets/wikiauto_train.csv               87.07 MB\n",
      "✓ hpc_datasets/asset_validation.csv              0.41 MB\n",
      "✓ hpc_datasets/asset_test.csv                    0.08 MB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify files and show their sizes\n",
    "import os\n",
    "\n",
    "print(\"Dataset Files Summary:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "files_to_check = [\n",
    "    \"bea_train.csv\",\n",
    "    \"bea_dev.csv\",\n",
    "    \"hpc_datasets/jfleg_test.csv\",\n",
    "    \"hpc_datasets/wikiauto_train.csv\",\n",
    "    \"hpc_datasets/asset_validation.csv\",\n",
    "    \"hpc_datasets/asset_test.csv\"\n",
    "]\n",
    "\n",
    "for filepath in files_to_check:\n",
    "    if os.path.exists(filepath):\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        size_mb = size_bytes / (1024 * 1024)\n",
    "        print(f\"✓ {filepath:40} {size_mb:10.2f} MB\")\n",
    "    else:\n",
    "        print(f\"✗ {filepath:40} NOT FOUND\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004251e",
   "metadata": {},
   "source": [
    "## Summary - Files Ready for HPC Transfer\n",
    "\n",
    "All datasets have been saved successfully:\n",
    "\n",
    "### Root Directory:\n",
    "- `bea_train.csv` - BEA-2019 training set (68,616 examples)\n",
    "- `bea_dev.csv` - BEA-2019 validation set (8,768 examples)\n",
    "\n",
    "### hpc_datasets/ Directory:\n",
    "- `jfleg_test.csv` - JFLEG test set (748 examples)\n",
    "- `wikiauto_train.csv` - WikiAuto training set (373,801 examples)\n",
    "- `asset_validation.csv` - ASSET validation set (2,000 examples)\n",
    "- `asset_test.csv` - ASSET test set (359 examples)\n",
    "\n",
    "**Total Size:** ~101.77 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5169786",
   "metadata": {},
   "source": [
    "# Clean Spaced Hyphens (Fix Tokenization Artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3c1d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning spaced hyphens from datasets...\n",
      "======================================================================\n",
      "\n",
      "Processing: data/bea_train.csv\n",
      "  ✓ Cleaned 5318 instances of ' - '\n",
      "  ✓ Remaining instances: 0\n",
      "  ✓ Saved cleaned version to data/bea_train.csv\n",
      "\n",
      "Processing: data/bea_dev.csv\n",
      "  ✓ Cleaned 778 instances of ' - '\n",
      "  ✓ Remaining instances: 0\n",
      "  ✓ Saved cleaned version to data/bea_dev.csv\n",
      "\n",
      "Processing: hpc_datasets/jfleg_test.csv\n",
      "  ✓ Cleaned 4 instances of ' - '\n",
      "  ✓ Remaining instances: 0\n",
      "  ✓ Saved cleaned version to hpc_datasets/jfleg_test.csv\n",
      "\n",
      "Processing: hpc_datasets/wikiauto_train.csv\n",
      "  ✓ Cleaned 3011 instances of ' - '\n",
      "  ✓ Remaining instances: 0\n",
      "  ✓ Saved cleaned version to hpc_datasets/wikiauto_train.csv\n",
      "\n",
      "Processing: hpc_datasets/asset_validation.csv\n",
      "  ✓ Cleaned 57 instances of ' - '\n",
      "  ✓ Remaining instances: 0\n",
      "  ✓ Saved cleaned version to hpc_datasets/asset_validation.csv\n",
      "\n",
      "Processing: hpc_datasets/asset_test.csv\n",
      "  ✓ Cleaned 2 instances of ' - '\n",
      "  ✓ Remaining instances: 0\n",
      "  ✓ Saved cleaned version to hpc_datasets/asset_test.csv\n",
      "\n",
      "======================================================================\n",
      "All datasets cleaned successfully!\n",
      "\n",
      "Example transformations:\n",
      "  'medium - sized' → 'medium-sized'\n",
      "  'high - density' → 'high-density'\n",
      "  'well - known' → 'well-known'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Function to clean spaced hyphens from text\n",
    "def clean_spaced_hyphens(text):\n",
    "    \"\"\"Replace ' - ' with '-' to fix tokenization artifacts\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.replace(' - ', '-')\n",
    "\n",
    "# List of all CSV files to clean\n",
    "csv_files = [\n",
    "    'data/bea_train.csv',\n",
    "    'data/bea_dev.csv',\n",
    "    'hpc_datasets/jfleg_test.csv',\n",
    "    'hpc_datasets/wikiauto_train.csv',\n",
    "    'hpc_datasets/asset_validation.csv',\n",
    "    'hpc_datasets/asset_test.csv'\n",
    "]\n",
    "\n",
    "print(\"Cleaning spaced hyphens from datasets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"\\n⚠ Skipping {csv_file} (not found)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing: {csv_file}\")\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Count occurrences before cleaning\n",
    "    before_count = df.astype(str).apply(lambda x: x.str.contains(' - ', regex=False).sum()).sum()\n",
    "    \n",
    "    # Clean both input and target columns\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(clean_spaced_hyphens)\n",
    "    \n",
    "    # Count occurrences after cleaning\n",
    "    after_count = df.astype(str).apply(lambda x: x.str.contains(' - ', regex=False).sum()).sum()\n",
    "    \n",
    "    # Save the cleaned CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"  ✓ Cleaned {before_count} instances of ' - '\")\n",
    "    print(f\"  ✓ Remaining instances: {after_count}\")\n",
    "    print(f\"  ✓ Saved cleaned version to {csv_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All datasets cleaned successfully!\")\n",
    "print(\"\\nExample transformations:\")\n",
    "print(\"  'medium - sized' → 'medium-sized'\")\n",
    "print(\"  'high - density' → 'high-density'\")\n",
    "print(\"  'well - known' → 'well-known'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a225ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification - First 3 rows of cleaned BEA train set:\n",
      "====================================================================================================\n",
      "\n",
      "Row 1:\n",
      "  Input:  My town is a medium size city with eighty thousand inhabitants ....\n",
      "  Target: My town is a medium-sized city with eighty thousand inhabitants ....\n",
      "\n",
      "Row 2:\n",
      "  Input:  It has a high density population because its small territory ....\n",
      "  Target: It has a high-density population because of its small territory ....\n",
      "\n",
      "Row 3:\n",
      "  Input:  Despite of it is an industrial city , there are many shops and department stores ....\n",
      "  Target: Although it is an industrial city , there are many shops and department stores ....\n",
      "\n",
      "====================================================================================================\n",
      "✓ Remaining spaced hyphens in bea_train.csv: 0\n",
      "✓ Total examples: 68616\n"
     ]
    }
   ],
   "source": [
    "# Verify the fix - check the specific example\n",
    "df = pd.read_csv('data/bea_train.csv')\n",
    "print(\"Verification - First 3 rows of cleaned BEA train set:\")\n",
    "print(\"=\" * 100)\n",
    "for idx in range(3):\n",
    "    print(f\"\\nRow {idx + 1}:\")\n",
    "    print(f\"  Input:  {df.iloc[idx]['input_text'][:100]}...\")\n",
    "    print(f\"  Target: {df.iloc[idx]['target_text'][:100]}...\")\n",
    "    \n",
    "# Check for any remaining spaced hyphens\n",
    "remaining = df.astype(str).apply(lambda x: x.str.contains(' - ', regex=False).sum()).sum()\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(f\"✓ Remaining spaced hyphens in bea_train.csv: {remaining}\")\n",
    "print(f\"✓ Total examples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad172e",
   "metadata": {},
   "source": [
    "## Cleaning Summary\n",
    "\n",
    "All datasets have been cleaned to remove spaced hyphens (` - ` → `-`):\n",
    "\n",
    "| Dataset | Instances Cleaned |\n",
    "|---------|------------------|\n",
    "| BEA train | 5,318 |\n",
    "| BEA dev | 778 |\n",
    "| JFLEG test | 4 |\n",
    "| WikiAuto train | 3,011 |\n",
    "| ASSET validation | 57 |\n",
    "| ASSET test | 2 |\n",
    "| **Total** | **9,170** |\n",
    "\n",
    "This ensures consistent formatting for LLM comparison without needing extra prompt engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
