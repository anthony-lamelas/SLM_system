{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e404604d",
   "metadata": {},
   "source": [
    "# Load and Clean Datasets\n",
    "\n",
    "This notebook loads datasets from HuggingFace, cleans them (removes spaced hyphens), and saves them to the `hpc_datasets/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4dcb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\SLM_system\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning datasets...\n",
      "======================================================================\n",
      "\n",
      "Loading BEA-2019 dataset...\n",
      "⚠ BEA-2019 data directory not found. Checked:\n",
      "  - wi+locness_v2.1.bea19/wi+locness/m2\n",
      "  - data/wi_locness/m2\n",
      "  - wi_locness/m2\n",
      "  Please download W&I+LOCNESS v2.1 from:\n",
      "  https://www.cl.cam.ac.uk/research/nl/bea2019st/#data\n",
      "  and extract to the project root directory\n",
      "\n",
      "Loading JFLEG dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\SLM_system\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lambe\\.cache\\huggingface\\hub\\datasets--jfleg. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating validation split: 100%|██████████| 755/755 [00:00<00:00, 126830.32 examples/s]\n",
      "Generating test split: 100%|██████████| 748/748 [00:00<00:00, 276344.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved JFLEG test set (748 examples) to hpc_datasets/jfleg_test.csv\n",
      "\n",
      "Loading WikiAuto dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Coding\\SLM_system\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lambe\\.cache\\huggingface\\hub\\datasets--chaojiang06--wiki_auto. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 373801 examples [00:00, 2545583.45 examples/s]\n",
      "Generating validation split: 73249 examples [00:00, 2521491.20 examples/s]\n",
      "Generating test split: 118074 examples [00:00, 2636995.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved WikiAuto train set (373801 examples) to hpc_datasets/wikiauto_train.csv\n",
      "\n",
      "Loading ASSET dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\SLM_system\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lambe\\.cache\\huggingface\\hub\\datasets--asset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 405305.50 examples/s]\n",
      "Generating test split: 100%|██████████| 359/359 [00:00<00:00, 105201.92 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved ASSET validation set (2000 examples) to hpc_datasets/asset_validation.csv\n",
      "✓ Saved ASSET test set (359 examples) to hpc_datasets/asset_test.csv\n",
      "\n",
      "======================================================================\n",
      "All datasets saved successfully to hpc_datasets/ directory!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create hpc_datasets directory if it doesn't exist\n",
    "os.makedirs(\"hpc_datasets\", exist_ok=True)\n",
    "\n",
    "# Function to clean spaced hyphens from text\n",
    "def clean_spaced_hyphens(text):\n",
    "    \"\"\"Replace ' - ' with '-' to fix tokenization artifacts\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.replace(' - ', '-')\n",
    "\n",
    "# Parse an \"A ...\" line from .m2\n",
    "A_RE = re.compile(r\"^A (\\d+) (\\d+)\\|\\|\\|[^|]*\\|\\|\\|([^|]*)\\|\\|\\|\")\n",
    "\n",
    "def apply_edits(src):\n",
    "    toks = src.split()\n",
    "    # apply collected edits (right→left so indices stay valid)\n",
    "    for s,e,repl in sorted(apply_edits.edits, key=lambda x: x[0], reverse=True):\n",
    "        repl_toks = [] if repl in (\"\", \"-NONE-\") else repl.split()\n",
    "        toks[s:e] = repl_toks\n",
    "    return \" \".join(toks)\n",
    "apply_edits.edits = []  # static holder\n",
    "\n",
    "def m2_to_pairs(path):\n",
    "    pairs, src = [], None\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\"S \"):\n",
    "                # flush previous\n",
    "                if src is not None:\n",
    "                    tgt = apply_edits(src)\n",
    "                    pairs.append((src, tgt))\n",
    "                src = line[2:]\n",
    "                apply_edits.edits = []\n",
    "            elif line.startswith(\"A \"):\n",
    "                m = A_RE.match(line)\n",
    "                if m:\n",
    "                    s, e, repl = int(m.group(1)), int(m.group(2)), m.group(3).strip()\n",
    "                    apply_edits.edits.append((s, e, repl))\n",
    "            elif line == \"\":  # sentence boundary\n",
    "                if src is not None:\n",
    "                    tgt = apply_edits(src)\n",
    "                    pairs.append((src, tgt))\n",
    "                    src = None\n",
    "                    apply_edits.edits = []\n",
    "    # tail\n",
    "    if src is not None:\n",
    "        tgt = apply_edits(src)\n",
    "        pairs.append((src, tgt))\n",
    "    return pairs\n",
    "\n",
    "print(\"Loading and cleaning datasets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. BEA-2019 train and validation sets\n",
    "print(\"\\nLoading BEA-2019 dataset...\")\n",
    "# Check multiple possible locations\n",
    "possible_dirs = [\n",
    "    \"wi+locness_v2.1.bea19/wi+locness/m2\",\n",
    "    \"data/wi_locness/m2\",\n",
    "    \"wi_locness/m2\"\n",
    "]\n",
    "m2_dir = None\n",
    "for dir_path in possible_dirs:\n",
    "    if os.path.exists(dir_path):\n",
    "        m2_dir = dir_path\n",
    "        break\n",
    "\n",
    "if m2_dir:\n",
    "    train, dev = [], []\n",
    "    for fname in os.listdir(m2_dir): \n",
    "        if fname.endswith(\".m2\"):\n",
    "            path = os.path.join(m2_dir, fname)\n",
    "            if \"train\" in fname.lower():\n",
    "                train += m2_to_pairs(path)\n",
    "            elif \"dev\" in fname.lower():\n",
    "                dev += m2_to_pairs(path)\n",
    "    \n",
    "    if train:\n",
    "        bea_train_df = pd.DataFrame(train, columns=[\"input_text\", \"target_text\"])\n",
    "        # Clean spaced hyphens\n",
    "        for col in bea_train_df.columns:\n",
    "            bea_train_df[col] = bea_train_df[col].apply(clean_spaced_hyphens)\n",
    "        bea_train_df.to_csv(\"hpc_datasets/bea_train.csv\", index=False)\n",
    "        print(f\"✓ Saved BEA-2019 train set ({len(bea_train_df)} examples) to hpc_datasets/bea_train.csv\")\n",
    "    else:\n",
    "        print(\"⚠ No BEA train files found in .m2 files\")\n",
    "    \n",
    "    if dev:\n",
    "        bea_dev_df = pd.DataFrame(dev, columns=[\"input_text\", \"target_text\"])\n",
    "        # Clean spaced hyphens\n",
    "        for col in bea_dev_df.columns:\n",
    "            bea_dev_df[col] = bea_dev_df[col].apply(clean_spaced_hyphens)\n",
    "        bea_dev_df.to_csv(\"hpc_datasets/bea_dev.csv\", index=False)\n",
    "        print(f\"✓ Saved BEA-2019 validation set ({len(bea_dev_df)} examples) to hpc_datasets/bea_dev.csv\")\n",
    "    else:\n",
    "        print(\"⚠ No BEA dev files found in .m2 files\")\n",
    "else:\n",
    "    print(f\"⚠ BEA-2019 data directory not found. Checked:\")\n",
    "    for dir_path in possible_dirs:\n",
    "        print(f\"  - {dir_path}\")\n",
    "    print(\"  Please download W&I+LOCNESS v2.1 from:\")\n",
    "    print(\"  https://www.cl.cam.ac.uk/research/nl/bea2019st/#data\")\n",
    "    print(\"  and extract to the project root directory\")\n",
    "\n",
    "# 2. JFLEG test set\n",
    "print(\"\\nLoading JFLEG dataset...\")\n",
    "jfleg = load_dataset(\"jfleg\", split=\"test\")\n",
    "jfleg_df = pd.DataFrame({\n",
    "    \"input_text\": jfleg[\"sentence\"],\n",
    "    \"target_text\": [refs[0] for refs in jfleg[\"corrections\"]]  # Using first correction\n",
    "})\n",
    "# Clean spaced hyphens\n",
    "for col in jfleg_df.columns:\n",
    "    jfleg_df[col] = jfleg_df[col].apply(clean_spaced_hyphens)\n",
    "jfleg_df.to_csv(\"hpc_datasets/jfleg_test.csv\", index=False)\n",
    "print(f\"✓ Saved JFLEG test set ({len(jfleg_df)} examples) to hpc_datasets/jfleg_test.csv\")\n",
    "\n",
    "# 3. WikiAuto train set\n",
    "print(\"\\nLoading WikiAuto dataset...\")\n",
    "wiki_auto = load_dataset(\n",
    "    \"chaojiang06/wiki_auto\",\n",
    "    \"default\",\n",
    "    revision=\"refs/convert/parquet\"\n",
    ")\n",
    "wiki_train_df = pd.DataFrame({\n",
    "    \"input_text\": wiki_auto[\"train\"][\"normal_sentence\"],\n",
    "    \"target_text\": wiki_auto[\"train\"][\"simple_sentence\"]\n",
    "})\n",
    "# Clean spaced hyphens\n",
    "for col in wiki_train_df.columns:\n",
    "    wiki_train_df[col] = wiki_train_df[col].apply(clean_spaced_hyphens)\n",
    "wiki_train_df.to_csv(\"hpc_datasets/wikiauto_train.csv\", index=False)\n",
    "print(f\"✓ Saved WikiAuto train set ({len(wiki_train_df)} examples) to hpc_datasets/wikiauto_train.csv\")\n",
    "\n",
    "# 4. ASSET validation set\n",
    "print(\"\\nLoading ASSET dataset...\")\n",
    "asset = load_dataset(\"asset\")\n",
    "asset_val_df = pd.DataFrame({\n",
    "    \"input_text\": asset[\"validation\"][\"original\"],\n",
    "    \"target_text\": [refs[0] for refs in asset[\"validation\"][\"simplifications\"]]  # Using first simplification\n",
    "})\n",
    "# Clean spaced hyphens\n",
    "for col in asset_val_df.columns:\n",
    "    asset_val_df[col] = asset_val_df[col].apply(clean_spaced_hyphens)\n",
    "asset_val_df.to_csv(\"hpc_datasets/asset_validation.csv\", index=False)\n",
    "print(f\"✓ Saved ASSET validation set ({len(asset_val_df)} examples) to hpc_datasets/asset_validation.csv\")\n",
    "\n",
    "# 5. ASSET test set\n",
    "asset_test_df = pd.DataFrame({\n",
    "    \"input_text\": asset[\"test\"][\"original\"],\n",
    "    \"target_text\": [refs[0] for refs in asset[\"test\"][\"simplifications\"]]  # Using first simplification\n",
    "})\n",
    "# Clean spaced hyphens\n",
    "for col in asset_test_df.columns:\n",
    "    asset_test_df[col] = asset_test_df[col].apply(clean_spaced_hyphens)\n",
    "asset_test_df.to_csv(\"hpc_datasets/asset_test.csv\", index=False)\n",
    "print(f\"✓ Saved ASSET test set ({len(asset_test_df)} examples) to hpc_datasets/asset_test.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All datasets saved successfully to hpc_datasets/ directory!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
